{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"fast5_rekindler","text":"<p>Documentation: https://adnaniazi.github.io/fast5_rekindler</p> <p>Source Code: https://github.com/adnaniazi/fast5_rekindler</p> <p>PyPI: https://pypi.org/project/fast5_rekindler/</p> <p>Collates information from BAM and POD5 files and generates FAST5 files for use in legacy tools such as tailfindr.</p>"},{"location":"#installation","title":"Installation","text":"<p>1. Create Python 3.10 or 3.11 environment.</p> <p><code>bash  conda create -n f5r python=3.11</code></p> <p>2. Activate the environment.</p> <p><code>bash  conda activate f5r</code></p> <p>3. Install FAST5 Rekindler.</p> <p><code>bash  pip install fast5_rekindler</code></p>"},{"location":"#usage","title":"Usage","text":"<p>FAST5 rekindler needs:</p> <p>1. A BAM file with <code>moves</code> table in it.</p> <p>You can generate it using Dorado:</p> <pre><code>dorado basecaller /path/to/basecalling/model \\\n  /pod5/dir/path \\\n  --recursive  \\\n  --emit-sam  \\\n  --emit-moves  \\\n  --device \"cpu\"  \\ # or \"cuda:all\"\n  --reference /path/to/alginment/reference &gt; /path/to/calls.sam\n</code></pre> <p>2. Convert Doarado's output SAM file to a BAM file.</p> <pre><code>samtools view -bS /path/to/calls.sam &gt; /path/to/calls.bam\n</code></pre> <p>3. Sort the BAM file.</p> <pre><code>samtools sort /path/to/calls.bam -o /path/to/sorted.calls.bam\n</code></pre> <p>4. Use FAST5 Rekindler to convert POD5 files to FAST5 files.</p> <pre><code>fast5_rekindler /path/to/sorted.calls.bam  \\\n  /path/to/pod5_dir \\\n  /path/to/output_dir \\\n  --num_processes 100\n</code></pre> <p>To invoke help for FAST5 Rekindler, just type:</p> <pre><code>fast5_rekindler --help\n</code></pre>"},{"location":"#development","title":"Development","text":"<ul> <li>Clone this repository</li> <li>Requirements:</li> <li>Poetry</li> <li>Python 3.7+</li> <li>Create a virtual environment and install the dependencies</li> </ul> <pre><code>poetry install\n</code></pre> <ul> <li>Activate the virtual environment</li> </ul> <pre><code>poetry shell\n</code></pre>"},{"location":"#testing","title":"Testing","text":"<pre><code>pytest\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>The documentation is automatically generated from the content of the docs directory and from the docstrings  of the public signatures of the source code. The documentation is updated and published as a Github project page   automatically as part each release.</p>"},{"location":"#releasing","title":"Releasing","text":"<p>Trigger the Draft release workflow (press Run workflow). This will update the changelog &amp; version and create a GitHub release which is in Draft state.</p> <p>Find the draft release from the GitHub releases and publish it. When  a release is published, it'll trigger release workflow which creates PyPI  release and deploys updated documentation.</p>"},{"location":"#pre-commit","title":"Pre-commit","text":"<p>Pre-commit hooks run all the auto-formatters (e.g. <code>black</code>, <code>isort</code>), linters (e.g. <code>mypy</code>, <code>flake8</code>), and other quality  checks to make sure the changeset is in good shape before a commit/push happens.</p> <p>You can install the hooks with (runs for each commit):</p> <pre><code>pre-commit install\n</code></pre> <p>Or if you want them to run only for each push:</p> <pre><code>pre-commit install -t pre-push\n</code></pre> <p>Or if you want e.g. want to run all checks manually for all files:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"api_docs/","title":"API documentation","text":""},{"location":"api_docs/#src.fast5_rekindler.bam","title":"<code>bam</code>","text":"<p>Yields records from a BAM file using pysam</p>"},{"location":"api_docs/#src.fast5_rekindler.bam.DatabaseHandler","title":"<code>DatabaseHandler</code>","text":"<p>Database handler for multiprocessing.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>class DatabaseHandler:\n    \"\"\"Database handler for multiprocessing.\"\"\"\n\n    def __init__(self, output_dir: str, num_processes: int):\n        \"\"\"Initializes the database handler.\"\"\"\n        self.output_dir = output_dir\n        self.num_processes = num_processes\n\n    def init_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Initializes the database for each worker.\"\"\"\n        unique_db_path = os.path.join(self.output_dir, f\"tmp_worker_{worker_id}.db\")\n        worker_state[\"db_connection\"] = sqlite3.connect(unique_db_path)\n        worker_state[\"db_cursor\"] = worker_state[\"db_connection\"].cursor()\n        worker_state[\"db_cursor\"].execute(TABLE_INIT_QUERY)\n        worker_state[\"db_connection\"].commit()\n\n    def exit_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Closes the database connection for each worker.\"\"\"\n        conn = worker_state[\"db_connection\"]\n        cursor = worker_state[\"db_cursor\"]\n        conn.commit()\n        cursor.close()\n        conn.close()\n\n    def merge_databases(self) -&gt; None:\n        \"\"\"Merges the databases from each worker into a single database.\"\"\"\n        db_path = os.path.join(self.output_dir, \"bam_db.db\")\n        main_conn = sqlite3.connect(db_path)\n        main_cursor = main_conn.cursor()\n        logger.info(\"Merging databases from each worker into a single database...\")\n        main_cursor.execute(TABLE_INIT_QUERY)\n\n        try:\n            for i in range(self.num_processes):\n                worker_db_path = os.path.join(self.output_dir, f\"tmp_worker_{i}.db\")\n\n                try:\n                    main_cursor.execute(\n                        f\"ATTACH DATABASE '{worker_db_path}' AS worker_db\"\n                    )\n                    main_cursor.execute(\"BEGIN\")\n\n                    main_cursor.execute(\n                        \"\"\"\n                        INSERT OR IGNORE INTO bam_db\n                        SELECT * FROM worker_db.bam_db\n                        \"\"\"\n                    )\n\n                    main_cursor.execute(\"COMMIT\")\n\n                except Exception:\n                    logger.warning(\n                        \"May be an empty database # {}. Nothing to worry about.\",\n                        i,\n                    )\n\n                finally:\n                    main_cursor.execute(\"DETACH DATABASE worker_db\")\n                    os.remove(worker_db_path)\n\n        finally:\n            main_cursor.close()\n            main_conn.close()\n            logger.info(\"Merging databases complete.\")\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.DatabaseHandler.__init__","title":"<code>__init__(output_dir: str, num_processes: int)</code>","text":"<p>Initializes the database handler.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def __init__(self, output_dir: str, num_processes: int):\n    \"\"\"Initializes the database handler.\"\"\"\n    self.output_dir = output_dir\n    self.num_processes = num_processes\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.DatabaseHandler.exit_func","title":"<code>exit_func(worker_id: int, worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Closes the database connection for each worker.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def exit_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Closes the database connection for each worker.\"\"\"\n    conn = worker_state[\"db_connection\"]\n    cursor = worker_state[\"db_cursor\"]\n    conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.DatabaseHandler.init_func","title":"<code>init_func(worker_id: int, worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Initializes the database for each worker.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def init_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Initializes the database for each worker.\"\"\"\n    unique_db_path = os.path.join(self.output_dir, f\"tmp_worker_{worker_id}.db\")\n    worker_state[\"db_connection\"] = sqlite3.connect(unique_db_path)\n    worker_state[\"db_cursor\"] = worker_state[\"db_connection\"].cursor()\n    worker_state[\"db_cursor\"].execute(TABLE_INIT_QUERY)\n    worker_state[\"db_connection\"].commit()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.DatabaseHandler.merge_databases","title":"<code>merge_databases() -&gt; None</code>","text":"<p>Merges the databases from each worker into a single database.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def merge_databases(self) -&gt; None:\n    \"\"\"Merges the databases from each worker into a single database.\"\"\"\n    db_path = os.path.join(self.output_dir, \"bam_db.db\")\n    main_conn = sqlite3.connect(db_path)\n    main_cursor = main_conn.cursor()\n    logger.info(\"Merging databases from each worker into a single database...\")\n    main_cursor.execute(TABLE_INIT_QUERY)\n\n    try:\n        for i in range(self.num_processes):\n            worker_db_path = os.path.join(self.output_dir, f\"tmp_worker_{i}.db\")\n\n            try:\n                main_cursor.execute(\n                    f\"ATTACH DATABASE '{worker_db_path}' AS worker_db\"\n                )\n                main_cursor.execute(\"BEGIN\")\n\n                main_cursor.execute(\n                    \"\"\"\n                    INSERT OR IGNORE INTO bam_db\n                    SELECT * FROM worker_db.bam_db\n                    \"\"\"\n                )\n\n                main_cursor.execute(\"COMMIT\")\n\n            except Exception:\n                logger.warning(\n                    \"May be an empty database # {}. Nothing to worry about.\",\n                    i,\n                )\n\n            finally:\n                main_cursor.execute(\"DETACH DATABASE worker_db\")\n                os.remove(worker_db_path)\n\n    finally:\n        main_cursor.close()\n        main_conn.close()\n        logger.info(\"Merging databases complete.\")\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.build_bam_db","title":"<code>build_bam_db(bam_filepath: str, output_dir: str, num_processes: int) -&gt; None</code>","text":"<p>Builds a database from a BAM file.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output directory.</p> required <code>num_processes</code> <code>int</code> <p>Number of processes to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def build_bam_db(bam_filepath: str, output_dir: str, num_processes: int) -&gt; None:\n    \"\"\"Builds a database from a BAM file.\n\n    Params:\n        bam_filepath (str): Path to the BAM file.\n        output_dir (str): Path to the output directory.\n        num_processes (int): Number of processes to use.\n\n    Returns:\n        None\n    \"\"\"\n    num_bam_records = get_total_records(bam_filepath)\n    db_handler = DatabaseHandler(output_dir, num_processes)\n    with WorkerPool(\n        n_jobs=num_processes, use_worker_state=True, pass_worker_id=True\n    ) as pool:\n        pool.map(\n            insert_bamdata_in_db_worker,\n            [(bam_data,) for bam_data in process_bam_records(bam_filepath)],\n            iterable_len=num_bam_records,\n            worker_init=db_handler.init_func,\n            worker_exit=db_handler.exit_func,\n            progress_bar=True,\n            progress_bar_options={\"desc\": \"\", \"unit\": \"records\", \"colour\": \"green\"},\n        )\n    db_handler.merge_databases()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.get_signal_info","title":"<code>get_signal_info(record: pysam.AlignedSegment) -&gt; Dict[str, Any]</code>","text":"<p>Returns a dictionary containing the signal information from a BAM record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>AlignedSegment</code> <p>A BAM record.</p> required <p>Returns:</p> Name Type Description <code>signal_info</code> <code>Dict[str, Any]</code> <p>A dictionary containing the signal information.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def get_signal_info(record: pysam.AlignedSegment) -&gt; Dict[str, Any]:\n    \"\"\"Returns a dictionary containing the signal information from a BAM record.\n\n    Params:\n        record (pysam.AlignedSegment): A BAM record.\n\n    Returns:\n        signal_info (Dict[str, Any]): A dictionary containing the signal information.\n    \"\"\"\n    signal_info = {}\n    tags_dict = dict(record.tags)  # type: ignore\n    try:\n        signal_info[\"moves_table\"] = tags_dict[\"mv\"]\n    except KeyError:\n        logger.exception(\n            \"The BAM file does not contain moves information. Please use --emit-moves option when basecalling using Doardo.\"\n        )\n        raise SystemExit\n    signal_info[\"moves_step\"] = signal_info[\"moves_table\"].pop(0)\n    signal_info[\"read_id\"] = record.query_name\n    signal_info[\"start_sample\"] = tags_dict[\"ts\"]\n    signal_info[\"num_samples\"] = tags_dict[\"ns\"]\n    signal_info[\"quality_score\"] = tags_dict[\"qs\"]\n    signal_info[\"channel\"] = tags_dict[\"ch\"]\n    signal_info[\"signal_mean\"] = tags_dict[\"sm\"]\n    signal_info[\"signal_sd\"] = tags_dict[\"sd\"]\n    signal_info[\"is_qcfail\"] = record.is_qcfail\n    signal_info[\"is_reverse\"] = record.is_reverse\n    signal_info[\"is_forward\"] = record.is_forward  # type: ignore\n    signal_info[\"is_mapped\"] = record.is_mapped  # type: ignore\n    signal_info[\"is_supplementary\"] = record.is_supplementary\n    signal_info[\"is_secondary\"] = record.is_secondary\n    signal_info[\"read_quality\"] = record.qual  # type: ignore\n    signal_info[\"read_fasta\"] = record.query_sequence\n    signal_info[\"mapping_quality\"] = record.mapping_quality\n    signal_info[\"parent_read_id\"] = tags_dict.get(\"pi\", \"\")\n    signal_info[\"split_point\"] = tags_dict.get(\"sp\", 0)\n    signal_info[\"time_stamp\"] = tags_dict.get(\"st\")\n\n    return signal_info\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.get_total_records","title":"<code>get_total_records(bam_filepath: str) -&gt; int</code>","text":"<p>Returns the total number of records in a BAM file.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <p>Returns:</p> Name Type Description <code>total_records</code> <code>int</code> <p>Total number of records in the BAM file.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def get_total_records(bam_filepath: str) -&gt; int:\n    \"\"\"Returns the total number of records in a BAM file.\n\n    Params:\n        bam_filepath (str): Path to the BAM file.\n\n    Returns:\n        total_records (int): Total number of records in the BAM file.\n    \"\"\"\n    with pysam.AlignmentFile(bam_filepath) as bam_file:\n        total_records = sum(1 for _ in bam_file)\n    return total_records\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.insert_bamdata_in_db_worker","title":"<code>insert_bamdata_in_db_worker(worker_id: int, worker_state: Dict[str, Any], bam_data: Dict[str, Any]) -&gt; None</code>","text":"<p>Inserts the signal information from a BAM record into the BAM database.</p> <p>Parameters:</p> Name Type Description Default <code>worker_id</code> <code>int</code> <p>Worker ID.</p> required <code>worker_state</code> <code>Dict[str, Any]</code> <p>Worker state.</p> required <code>bam_data</code> <code>Dict[str, Any]</code> <p>A dictionary containing the signal information.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def insert_bamdata_in_db_worker(\n    worker_id: int, worker_state: Dict[str, Any], bam_data: Dict[str, Any]\n) -&gt; None:\n    \"\"\"Inserts the signal information from a BAM record into the BAM database.\n\n    Params:\n        worker_id (int): Worker ID.\n        worker_state (Dict[str, Any]): Worker state.\n        bam_data (Dict[str, Any]): A dictionary containing the signal information.\n\n    Returns:\n        None\n    \"\"\"\n    cursor = worker_state[\"db_cursor\"]\n    if bam_data.get(\"is_secondary\"):\n        # Skip supplementary alignments as they do not contain FASTA information\n        # The FASTA information is only present in the primary alignment\n        # and is picked up from these record of the primary alignment\n        return\n\n    try:\n        # Extract the required attributes from the BAM data dictionary\n        block_stride = bam_data.get(\"moves_step\")\n        moves_table = bam_data.get(\"moves_table\")\n        called_events = len(moves_table) if moves_table is not None else 0\n        mean_qscore = bam_data.get(\"quality_score\")\n        read_fasta = bam_data.get(\"read_fasta\")\n        sequence_length = len(read_fasta) if read_fasta is not None else 0\n        duration_template = bam_data.get(\"num_samples\")\n        first_sample_template = bam_data.get(\"start_sample\")\n        num_events_template = called_events\n        moves_table = bam_data.get(\"moves_table\").tobytes()  # type: ignore\n        read_fasta = bam_data.get(\"read_fasta\")\n        read_quality = bam_data.get(\"read_quality\")\n        read_id = bam_data.get(\"read_id\")\n        parent_read_id = bam_data.get(\"parent_read_id\")\n        split_point = bam_data.get(\"split_point\")\n        time_stamp = bam_data.get(\"time_stamp\")\n\n        insert_query = \"\"\"INSERT OR IGNORE INTO bam_db (\n                            block_stride,\n                            called_events,\n                            mean_qscore,\n                            sequence_length,\n                            duration_template,\n                            first_sample_template,\n                            num_events_template,\n                            moves_table,\n                            read_fasta,\n                            read_quality,\n                            read_id,\n                            parent_read_id,\n                            split_point,\n                            time_stamp\n                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\"\"\n\n        cursor.execute(\n            insert_query,\n            (\n                block_stride,\n                called_events,\n                mean_qscore,\n                sequence_length,\n                duration_template,\n                first_sample_template,\n                num_events_template,\n                moves_table,\n                read_fasta,\n                read_quality,\n                read_id,\n                parent_read_id,\n                split_point,\n                time_stamp,\n            ),\n        )\n    except Exception:\n        logger.exception(\n            \"An error occurred in worker {}, read_id: {}\", worker_id, read_id\n        )\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.bam.process_bam_records","title":"<code>process_bam_records(bam_filepath: str) -&gt; Generator[Dict[str, Any], None, None]</code>","text":"<p>Yields records from a BAM file using pysam one-by-one and extracts the signal information from each of the record.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file.</p> required <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>signal_info Generator[Dict[str, Any], None, None]: A dictionary containing the signal information.</p> Source code in <code>src/fast5_rekindler/bam.py</code> <pre><code>def process_bam_records(bam_filepath: str) -&gt; Generator[Dict[str, Any], None, None]:\n    \"\"\"Yields records from a BAM file using pysam one-by-one and\n    extracts the signal information from each of the record.\n\n    Params:\n        bam_filepath (str): Path to the BAM file.\n\n    Yields:\n        signal_info Generator[Dict[str, Any], None, None]: A dictionary containing the signal information.\n\n    \"\"\"\n    index_filepath = f\"{bam_filepath}.bai\"\n\n    if not os.path.exists(index_filepath):\n        pysam.index(bam_filepath)  # type: ignore\n\n    with pysam.AlignmentFile(bam_filepath, \"rb\") as bam_file:\n        for record in bam_file:\n            yield get_signal_info(record)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.cleanup","title":"<code>cleanup</code>","text":""},{"location":"api_docs/#src.fast5_rekindler.cleanup.cleanup_fast5","title":"<code>cleanup_fast5(fast5_filepath: str) -&gt; None</code>","text":"<p>Delete configuration groups from a FAST5 file. These groups are added automatically by ont-fast5-api but are not needed in rekindled FAST5 files.</p> <p>Parameters:</p> Name Type Description Default <code>fast5_filepath</code> <code>str</code> <p>Path to a FAST5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/cleanup.py</code> <pre><code>def cleanup_fast5(fast5_filepath: str) -&gt; None:\n    \"\"\"\n    Delete configuration groups from a FAST5 file.\n    These groups are added automatically by ont-fast5-api\n    but are not needed in rekindled FAST5 files.\n\n    Params:\n        fast5_filepath (str): Path to a FAST5 file.\n\n    Returns:\n        None\n    \"\"\"\n    with h5py.File(fast5_filepath, \"r+\") as file:\n        delete_configuration_groups(file)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.cleanup.delete_configuration_groups","title":"<code>delete_configuration_groups(group: Any) -&gt; None</code>","text":"<p>Recursively delete groups named 'Configuration'.</p> <p>Parameters:</p> Name Type Description Default <code>group</code> <code>Group</code> <p>A group object.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/cleanup.py</code> <pre><code>def delete_configuration_groups(group: Any) -&gt; None:\n    \"\"\"\n    Recursively delete groups named 'Configuration'.\n\n    Params:\n        group (h5py.Group): A group object.\n\n    Returns:\n        None\n    \"\"\"\n    # Make a list of keys to iterate over to avoid RuntimeError for changing size during iteration\n    keys = list(group.keys())\n    for key in keys:\n        item = group[key]\n        if isinstance(item, h5py.Group):\n            # If the item is a group and its name is 'Configuration', delete it\n            if key == \"Configuration\":\n                del group[key]\n            else:\n                # Otherwise, recursively check its subgroups\n                delete_configuration_groups(item)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.cli","title":"<code>cli</code>","text":""},{"location":"api_docs/#src.fast5_rekindler.cli.fast5_rekindler","title":"<code>fast5_rekindler() -&gt; None</code>","text":"<p>CLI entry point for rekindle_fast5 package.</p> Source code in <code>src/fast5_rekindler/cli.py</code> <pre><code>def fast5_rekindler() -&gt; None:\n    \"\"\"CLI entry point for rekindle_fast5 package.\"\"\"\n    app_version = get_version()\n\n    # Use RawTextHelpFormatter to preserve newlines\n    parser = argparse.ArgumentParser(\n        description=\"Creates legacy basecalled FAST5 files by collating information in BAM and POD5 files.\",\n        formatter_class=argparse.RawTextHelpFormatter,\n        epilog=\"\"\"Example usage:\nfast5_rekindler /path/to/calls.bam /path/to/pod5_dir /path/to/output_dir -n 4\n        \"\"\",\n    )\n\n    # Positional arguments\n    parser.add_argument(\n        \"bam_filepath\",\n        type=str,\n        help=(\n            \"Path to the BAM file. \\n\"\n            \"This file must contain moves information. \\n\"\n            \"To add moves info to the BAM file, \\n\"\n            \"use --emit-moves option when basecalling using Doardo.\"\n        ),\n    )\n    parser.add_argument(\"pod5_dir\", type=str, help=\"Directory containing POD5 files.\")\n    parser.add_argument(\n        \"output_dir\",\n        type=str,\n        help=\"Directory where the output FAST5 files will be saved.\",\n    )\n\n    # Optional arguments (options)\n    parser.add_argument(\n        \"-n\",\n        \"--num-processes\",\n        type=int,\n        default=4,\n        metavar=\"\",\n        help=\"Number of processes to use. Default is 4.\",\n    )\n\n    # Version argument\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        action=\"version\",\n        version=f\"%(prog)s {app_version}\",\n        help=\"Show program's version number and exit\",\n    )  # Replace 1.0.0 with your actual version\n\n    args = parser.parse_args()\n\n    # Initialize the logger\n    log_filepath = configure_logger(new_log_directory=args.output_dir)\n\n    width = 15\n    full_command = \" \".join(sys.argv)\n    logger.info(\"You have issued the following command: {}\\n\", full_command)\n\n    logger.info(\n        \"\"\"You have configured the following options:\n        {} {}\n        {} {}\n        {} {}\n        {} {}\n        {} {}, \\n\"\"\",\n        \"\u2022 BAM filepath:\".ljust(width),\n        args.bam_filepath,\n        \"\u2022 POD5 dir:\".ljust(width),\n        args.pod5_dir,\n        \"\u2022 Output dir:\".ljust(width),\n        args.output_dir,\n        \"\u2022 Num CPUs:\".ljust(width),\n        args.num_processes,\n        \"\u2022 Log file:\".ljust(width),\n        log_filepath,\n    )\n\n    # Call the function with parsed arguments\n    collate_bam_fast5(\n        args.bam_filepath, args.pod5_dir, args.output_dir, args.num_processes\n    )\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate","title":"<code>collate</code>","text":""},{"location":"api_docs/#src.fast5_rekindler.collate.DatabaseHandler","title":"<code>DatabaseHandler</code>","text":"<p>Class that handles database connections for each worker. Each worker can do READ operations in parallel for fetching BAM information for a given read_id.</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>class DatabaseHandler:\n    \"\"\"Class that handles database connections for each worker.\n    Each worker can do READ operations in parallel for fetching\n    BAM information for a given read_id.\n    \"\"\"\n\n    def __init__(self, database_path: str) -&gt; None:\n        \"\"\"Initializes the database path.\n\n        Params:\n            database_path (str): Path to the database file.\n        \"\"\"\n        self.database_path = database_path\n\n    def open_database(self) -&gt; Tuple[sqlite3.Cursor, sqlite3.Connection]:\n        \"\"\"Connect to SQLite database (or create it if it doesn't exist)\"\"\"\n        conn = sqlite3.connect(self.database_path)\n        cursor = conn.cursor()\n        return cursor, conn\n\n    def init_func(self, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Initializes the database connection for each worker.\"\"\"\n        worker_state[\"db_cursor\"], worker_state[\"db_connection\"] = self.open_database()\n\n    def exit_func(self, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Closes the database connection for each worker.\"\"\"\n        conn = worker_state[\"db_connection\"]\n        cursor = worker_state[\"db_cursor\"]\n        cursor.close()\n        conn.close()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.DatabaseHandler.__init__","title":"<code>__init__(database_path: str) -&gt; None</code>","text":"<p>Initializes the database path.</p> <p>Parameters:</p> Name Type Description Default <code>database_path</code> <code>str</code> <p>Path to the database file.</p> required Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def __init__(self, database_path: str) -&gt; None:\n    \"\"\"Initializes the database path.\n\n    Params:\n        database_path (str): Path to the database file.\n    \"\"\"\n    self.database_path = database_path\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.DatabaseHandler.exit_func","title":"<code>exit_func(worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Closes the database connection for each worker.</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def exit_func(self, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Closes the database connection for each worker.\"\"\"\n    conn = worker_state[\"db_connection\"]\n    cursor = worker_state[\"db_cursor\"]\n    cursor.close()\n    conn.close()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.DatabaseHandler.init_func","title":"<code>init_func(worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Initializes the database connection for each worker.</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def init_func(self, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Initializes the database connection for each worker.\"\"\"\n    worker_state[\"db_cursor\"], worker_state[\"db_connection\"] = self.open_database()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.DatabaseHandler.open_database","title":"<code>open_database() -&gt; Tuple[sqlite3.Cursor, sqlite3.Connection]</code>","text":"<p>Connect to SQLite database (or create it if it doesn't exist)</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def open_database(self) -&gt; Tuple[sqlite3.Cursor, sqlite3.Connection]:\n    \"\"\"Connect to SQLite database (or create it if it doesn't exist)\"\"\"\n    conn = sqlite3.connect(self.database_path)\n    cursor = conn.cursor()\n    return cursor, conn\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.add_info_to_fast5_read","title":"<code>add_info_to_fast5_read(f5_read: Any, data: Dict[str, Any], read_id: str) -&gt; None</code>","text":"<p>Adds basecalling information the raw FAST5 read.</p> <p>Parameters:</p> Name Type Description Default <code>f5_read</code> <code>Any</code> <p>Raw FAST5 read object.</p> required <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing basecalling information.</p> required <code>read_id</code> <code>str</code> <p>Read ID.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def add_info_to_fast5_read(f5_read: Any, data: Dict[str, Any], read_id: str) -&gt; None:\n    \"\"\"Adds basecalling information the raw FAST5 read.\n\n    Params:\n        f5_read (Any): Raw FAST5 read object.\n        data (Dict[str, Any]): Dictionary containing basecalling information.\n        read_id (str): Read ID.\n\n    Returns:\n        None\n    \"\"\"\n    f5_read.add_analysis(\n        component=\"basecall_1d\",\n        group_name=\"Basecall_1D_000\",\n        attrs={\n            \"model_type\": \"flipflop\",\n            \"name\": \"ONT Guppy basecalling software.\",\n            \"segmentation\": \"Segmentation_000\",\n            \"time_stamp\": data.get(\"time_stamp\"),\n            \"version\": \"Unknow\",\n        },\n        config=None,\n    )\n    f5_read.add_analysis_subgroup(\n        group_name=\"Basecall_1D_000\", subgroup_name=\"BaseCalled_template\"\n    )\n    moves_array = np.frombuffer(data[\"moves_table\"], dtype=\"uint8\")\n    f5_read.add_analysis_dataset(\n        group_name=\"Basecall_1D_000/BaseCalled_template\",\n        dataset_name=\"Move\",\n        data=moves_array,\n        attrs=None,\n    )\n    fastq = (\n        \"@\"\n        + read_id\n        + \"\\n\"\n        + data[\"read_fasta\"]\n        + \"\\n\"\n        + \"+\"\n        + \"\\n\"\n        + data[\"read_quality\"]\n    )\n    f5_read.add_analysis_dataset(\n        group_name=\"Basecall_1D_000/BaseCalled_template\",\n        dataset_name=\"Fastq\",\n        data=fastq,\n        attrs=None,\n    )\n    f5_read.add_analysis_subgroup(\n        group_name=\"Basecall_1D_000/Summary\",\n        subgroup_name=\"basecall_1d_template\",\n        attrs={\n            \"block_stride\": data[\"block_stride\"],\n            \"called_events\": data[\"called_events\"],\n            \"mean_qscore\": data[\"mean_qscore\"],\n            \"scaling_mean_abs_dev\": 0,\n            \"scaling_median\": 0,\n            \"sequence_length\": data[\"sequence_length\"],\n            \"skip_prob\": 0,\n            \"stay_prob\": 0,\n            \"step_prob\": 0,\n            \"strand_score\": 0,\n        },\n    )\n    f5_read.add_analysis(\n        component=\"segmentation\",\n        group_name=\"Segmentation_000\",\n        attrs={\n            \"name\": \"ONT Guppy basecalling software.\",\n            \"time_stamp\": \"Unknown\",\n            \"version\": \"Unknown\",\n        },\n    )\n    f5_read.add_analysis_subgroup(\n        group_name=\"Segmentation_000/Summary\",\n        subgroup_name=\"segmentation\",\n        attrs={\n            \"adapter_max\": 0,\n            \"duration_template\": data[\"duration_template\"],\n            \"first_sample_template\": data[\"first_sample_template\"],\n            \"has_complement\": 0,\n            \"has_template\": 0,\n            \"med_abs_dev_template\": 0,\n            \"median_template\": 0,\n            \"num_events_template\": data[\"num_events_template\"],\n            \"pt_detect_success\": 0,\n            \"pt_median\": 0,\n            \"pt_sd\": 0,\n            \"parent_read_id\": data[\"parent_read_id\"],\n        },\n    )\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.collate_bam_fast5","title":"<code>collate_bam_fast5(bam_filepath: str, pod5_dir: str, output_dir: str, num_processes: int) -&gt; None</code>","text":"<p>Main function that combines information from BAM and all POD5 files to create legacy basecalled FAST5 files.</p> <p>Parameters:</p> Name Type Description Default <code>bam_filepath</code> <code>str</code> <p>Path to the BAM file. This file must contain moves information.</p> required <code>pod5_dir</code> <code>str</code> <p>Path to the directory containing POD5 files.</p> required <code>output_dir</code> <code>str</code> <p>Path to the directory where the output FAST5 files will be saved.               N.B.: The POD5 directory structure will be replicated in               the output directory.</p> required <code>num_processes</code> <code>int</code> <p>Number of processes to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def collate_bam_fast5(\n    bam_filepath: str, pod5_dir: str, output_dir: str, num_processes: int\n) -&gt; None:\n    \"\"\"Main function that combines information from BAM and all POD5 files to create\n    legacy basecalled FAST5 files.\n\n    Params:\n        bam_filepath (str): Path to the BAM file. This file must contain moves information.\n        pod5_dir (str): Path to the directory containing POD5 files.\n        output_dir (str): Path to the directory where the output FAST5 files will be saved.\n                          N.B.: The POD5 directory structure will be replicated in\n                          the output directory.\n        num_processes (int): Number of processes to use.\n\n    Returns:\n        None\n    \"\"\"\n    # 1. Convert POD5 to FAST5\n    logger.info(\"Step 1/4: Converting POD5 files to raw FAST5 files\")\n    convert_pod5_to_raw_fast5(pod5_dir, output_dir, num_processes)\n\n    # 2. Make index database\n    logger.info(\"Step 2/4: Building an index database\")\n    build_index_db(\n        fast5_dir=output_dir, output_dir=output_dir, num_processes=num_processes\n    )\n\n    # 3. Make BAM database\n    logger.info(\"Step 3/4: Reading BAM file info to the database\")\n    build_bam_db(bam_filepath, output_dir, num_processes)\n\n    # 4. Join databases\n    database_path = join_databases(output_dir)\n\n    # 5. Make a list of all FAST5 files\n    logger.info(\"Step 4/4: Writing basecalling information to FAST5 files\")\n    fast5_filepaths_list = make_fast5_files_list(output_dir)\n\n    db_handler = DatabaseHandler(database_path)\n\n    num_fast_files = len(fast5_filepaths_list)\n    with WorkerPool(\n        n_jobs=min(num_processes, num_fast_files), use_worker_state=True\n    ) as pool:\n        pool.map(\n            collate_bam_fast5_worker,\n            fast5_filepaths_list,\n            iterable_len=num_fast_files,\n            worker_init=db_handler.init_func,  # Passing method of db_handler object\n            worker_exit=db_handler.exit_func,  # Passing method of db_handler object\n            progress_bar=True,\n            progress_bar_options={\"desc\": \"\", \"unit\": \"files\", \"colour\": \"green\"},\n        )\n\n    os.remove(db_handler.database_path)\n    logger.success(\"Finished successfully!\")\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.collate_bam_fast5_worker","title":"<code>collate_bam_fast5_worker(worker_state: Dict[str, Any], fast5_filepath: str) -&gt; None</code>","text":"<p>Worker function that fetches information from BAM file and writes it to a raw FAST5 file in order to convert it into a basecalled FAST5 file. Unfortunately, some information such as the trace table can never be recreated due to a change in the basecaller design.</p> <p>Parameters:</p> Name Type Description Default <code>worker_state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the database connection and cursor.</p> required <code>fast5_filepath</code> <code>str</code> <p>Path to the raw FAST5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def collate_bam_fast5_worker(worker_state: Dict[str, Any], fast5_filepath: str) -&gt; None:\n    \"\"\"Worker function that fetches information from BAM file and writes it to a raw\n    FAST5 file in order to convert it into a basecalled FAST5 file. Unfortunately, some\n    information such as the trace table can never be recreated due to a change in the\n    basecaller design.\n\n    Params:\n        worker_state (Dict[str, Any]): Dictionary containing the database connection and cursor.\n        fast5_filepath (str): Path to the raw FAST5 file.\n\n    Returns:\n        None\n    \"\"\"\n    # First create extra reads for duplex/chimeric read that are split\n    # into two reads in the BAM file\n    create_duplex_reads_in_fast5(worker_state, fast5_filepath)\n\n    # Delete reads that are not in the BAM file\n    delete_nonexistant_reads_from_fast5(worker_state, fast5_filepath)\n\n    # Handle all the simplex/unfused reads\n    mf5 = MultiFast5File(fast5_filepath, mode=\"r+\")\n    read_ids_list = mf5.get_read_ids()\n\n    for read_id in read_ids_list:\n        f5_read = mf5.get_read(read_id)\n        try:\n            data = extract_simplex_data_from_db(\n                read_id,\n                \"read_id\",\n                worker_state=worker_state,\n                fetch_multiple=False,\n            )\n            if isinstance(data, dict):\n                add_info_to_fast5_read(f5_read, data, read_id)\n            else:\n                logger.error(\"Data for read_id {} is not in expected format\", read_id)\n        except Exception:\n            logger.debug(\"Error in read_id {} in file {}.\", read_id, fast5_filepath)\n    mf5.close()\n\n    # Delete configuration groups\n    cleanup_fast5(fast5_filepath)\n\n    # For duplex/chimeric reads, split the original raw signal properly between the two reads\n    # The splitted raw signal is not VBZ compressed\n    split_raw_signal_for_duplex_reads(worker_state, fast5_filepath)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.copy_group","title":"<code>copy_group(source_group: Any, target_group: Any) -&gt; None</code>","text":"<p>Recursively copies a group and its contents to a target group in an HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>source_group</code> <code>Any</code> <p>HDF5 source group.</p> required <code>target_group</code> <code>Any</code> <p>HDF5 arget group.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def copy_group(source_group: Any, target_group: Any) -&gt; None:\n    \"\"\"Recursively copies a group and its contents to a target group in an HDF5 file.\n\n    Params:\n        source_group (Any): HDF5 source group.\n        target_group (Any): HDF5 arget group.\n\n    Returns:\n        None\n    \"\"\"\n    # Copy attributes from the source group to the target group\n    for attr_name, attr_value in source_group.attrs.items():\n        if attr_name == \"read_id\":\n            attr_value = target_group.name.split(\"_\")[-1]\n        target_group.attrs[attr_name] = attr_value\n\n    # Recursively copy subgroups and datasets from the source group to the target group\n    for name, obj in source_group.items():\n        if isinstance(obj, h5py.Group):\n            new_subgroup = target_group.create_group(name)\n            copy_group(obj, new_subgroup)\n        elif isinstance(obj, h5py.Dataset):\n            # Copy the dataset to the target group\n            source_group.copy(obj, target_group, name=name)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.create_duplex_reads_in_fast5","title":"<code>create_duplex_reads_in_fast5(worker_state: Dict[str, Any], fast5_filepath: str) -&gt; None</code>","text":"<p>Creates extra reads for duplex/chimeric reads that are split into two reads in the BAM file. It copies all the information in the parent read group and renames it to the child read group.</p> <p>Parameters:</p> Name Type Description Default <code>worker_state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the database connection and cursor.</p> required <code>fast5_filepath</code> <code>str</code> <p>Path to the raw FAST5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def create_duplex_reads_in_fast5(\n    worker_state: Dict[str, Any], fast5_filepath: str\n) -&gt; None:\n    \"\"\"Creates extra reads for duplex/chimeric reads that are split into two reads in the BAM file.\n    It copies all the information in the parent read group and renames it to the child read group.\n\n    Params:\n        worker_state (Dict[str, Any]): Dictionary containing the database connection and cursor.\n        fast5_filepath (str): Path to the raw FAST5 file.\n\n    Returns:\n        None\n    \"\"\"\n    data = extract_read_data_from_db_based_on_action(\n        fast5_filepath, action=\"duplicate\", worker_state=worker_state\n    )\n\n    if data is None:\n        return\n\n    for record in data:\n        read_id = record.get(\"read_id\")\n        parent_read_id = record.get(\"parent_read_id\")\n        original_group_name = f\"/read_{parent_read_id}\"\n        new_group_name = f\"/read_{read_id}\"\n        duplicate_and_rename_group(fast5_filepath, original_group_name, new_group_name)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.delete_nonexistant_reads_from_fast5","title":"<code>delete_nonexistant_reads_from_fast5(worker_state: Dict[str, Any], fast5_filepath: str) -&gt; None</code>","text":"Deletes reads that are <p>a. Not in the BAM file but have no corresponding record in FAST5 file b. Duplex parent reads for which with no corresponding record in BAM file</p> <p>Parameters:</p> Name Type Description Default <code>worker_state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the database connection and cursor.</p> required <code>fast5_filepath</code> <code>str</code> <p>Path to the raw FAST5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def delete_nonexistant_reads_from_fast5(\n    worker_state: Dict[str, Any], fast5_filepath: str\n) -&gt; None:\n    \"\"\"Deletes reads that are:\n        a. Not in the BAM file but have no corresponding record in FAST5 file\n        b. Duplex parent reads for which with no corresponding record in BAM file\n\n    Params:\n        worker_state (Dict[str, Any]): Dictionary containing the database connection and cursor.\n        fast5_filepath (str): Path to the raw FAST5 file.\n\n    Returns:\n        None\n    \"\"\"\n    data_del = extract_read_data_from_db_based_on_action(\n        fast5_filepath, action=\"delete\", worker_state=worker_state\n    )\n\n    # Check if combined_data is still empty\n    if not data_del:\n        return\n\n    with h5py.File(fast5_filepath, \"r+\") as file:\n        for record in data_del:\n            read_id = record.get(\"read_id\")\n            group_name = f\"/read_{read_id}\"\n            # Use the `del` statement to delete the group\n            del file[group_name]\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.duplicate_and_rename_group","title":"<code>duplicate_and_rename_group(input_file: str, original_group_name: str, new_group_name: str) -&gt; None</code>","text":"<p>Duplicates a read group in an HDF5 file and renames it.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>Path to the HDF5 file.</p> required <code>original_group_name</code> <code>str</code> <p>Name of the original group.</p> required <code>new_group_name</code> <code>str</code> <p>Name of the new group.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def duplicate_and_rename_group(\n    input_file: str, original_group_name: str, new_group_name: str\n) -&gt; None:\n    \"\"\"Duplicates a read group in an HDF5 file and renames it.\n\n    Params:\n        input_file (str): Path to the HDF5 file.\n        original_group_name (str): Name of the original group.\n        new_group_name (str): Name of the new group.\n\n    Returns:\n        None\n    \"\"\"\n    with h5py.File(input_file, \"r+\") as file:\n        # Check if the original group exists\n        if original_group_name not in file:\n            print(f\"The group '{original_group_name}' does not exist in the HDF5 file.\")\n            return\n\n        # Read the original group\n        original_group = file[original_group_name]\n\n        # Create a new group with the desired new name\n        new_group = file.create_group(new_group_name)\n\n        # Copy contents of the original group to the new group\n        copy_group(original_group, new_group)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.extract_read_data_from_db_based_on_action","title":"<code>extract_read_data_from_db_based_on_action(fast5_filepath: str, action: str, worker_state: Dict[str, Any]) -&gt; Optional[List[Dict[str, Any]]]</code>","text":"<p>Extracts information from the database for a given FAST5 file based on a given action.</p> <p>Parameters:</p> Name Type Description Default <code>fast5_filepath</code> <code>str</code> <p>Path to the raw FAST5 file.</p> required <code>action</code> <code>str</code> <p>Action to query ('duplicate' or 'delete').</p> required <code>worker_state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the database connection and cursor.</p> required <p>Returns:</p> Type Description <code>Optional[List[Dict[str, Any]]]</code> <p>Optional[List[Dict[str, Any]]]: List of dictionaries containing BAM record information from the database.</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def extract_read_data_from_db_based_on_action(\n    fast5_filepath: str, action: str, worker_state: Dict[str, Any]\n) -&gt; Optional[List[Dict[str, Any]]]:\n    \"\"\"\n    Extracts information from the database for a given FAST5 file\n    based on a given action.\n\n    Params:\n        fast5_filepath (str): Path to the raw FAST5 file.\n        action (str): Action to query ('duplicate' or 'delete').\n        worker_state (Dict[str, Any]): Dictionary containing the database connection and cursor.\n\n    Returns:\n        Optional[List[Dict[str, Any]]]: List of dictionaries containing BAM record information from the database.\n    \"\"\"\n    cursor = worker_state.get(\"db_cursor\")\n    assert cursor is not None, \"Database cursor is unexpectedly None\"\n\n    query = \"\"\"SELECT\n                    read_id,\n                    parent_read_id,\n                    fast5_filepath,\n                    block_stride,\n                    called_events,\n                    mean_qscore,\n                    sequence_length,\n                    duration_template,\n                    first_sample_template,\n                    num_events_template,\n                    moves_table,\n                    read_fasta,\n                    read_quality,\n                    action,\n                    split_point\n                FROM fast5_bam_db\n                WHERE fast5_filepath = ? AND action = ?\n            \"\"\"\n\n    # Execute the query with the provided ID value\n    cursor.execute(query, (fast5_filepath, action))\n\n    # Define the columns\n    columns = [\n        \"read_id\",\n        \"parent_read_id\",\n        \"fast5_filepath\",\n        \"block_stride\",\n        \"called_events\",\n        \"mean_qscore\",\n        \"sequence_length\",\n        \"duration_template\",\n        \"first_sample_template\",\n        \"num_events_template\",\n        \"moves_table\",\n        \"read_fasta\",\n        \"read_quality\",\n        \"action\",\n        \"split_point\",\n    ]\n\n    rows = cursor.fetchall()\n\n    if not rows:\n        return None\n\n    return [dict(zip(columns, row)) for row in rows]\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.extract_simplex_data_from_db","title":"<code>extract_simplex_data_from_db(id_value: str, id_type: str, worker_state: Dict[str, Any], fetch_multiple: bool = False) -&gt; Union[Dict[str, Any], List[Dict[str, Any]], None]</code>","text":"<p>Extracts information from the database for a given read ID or parent read ID.</p> <p>Parameters:</p> Name Type Description Default <code>id_value</code> <code>str</code> <p>Value of the read ID or parent read ID.</p> required <code>id_type</code> <code>str</code> <p>Type of ID to query ('read_id' or 'parent_read_id').</p> required <code>worker_state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the database connection and cursor.</p> required <code>fetch_multiple</code> <code>bool</code> <p>Flag to fetch multiple records or a single record.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[Dict[str, Any], List[Dict[str, Any]], None]</code> <p>Union[Optional[Dict[str, Any]], Generator[Dict[str, Any], None, None]]:</p> <code>Union[Dict[str, Any], List[Dict[str, Any]], None]</code> <ul> <li>Dictionary containing BAM record information from the database, or None if no record is found.</li> </ul> <code>Union[Dict[str, Any], List[Dict[str, Any]], None]</code> <ul> <li>A generator that yields dictionaries containing BAM record information from the database.</li> </ul> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def extract_simplex_data_from_db(\n    id_value: str,\n    id_type: str,\n    worker_state: Dict[str, Any],\n    fetch_multiple: bool = False,\n) -&gt; Union[Dict[str, Any], List[Dict[str, Any]], None]:\n    \"\"\"\n    Extracts information from the database for a given read ID or parent read ID.\n\n    Params:\n        id_value (str): Value of the read ID or parent read ID.\n        id_type (str): Type of ID to query ('read_id' or 'parent_read_id').\n        worker_state (Dict[str, Any]): Dictionary containing the database connection and cursor.\n        fetch_multiple (bool): Flag to fetch multiple records or a single record.\n\n    Returns:\n        Union[Optional[Dict[str, Any]], Generator[Dict[str, Any], None, None]]:\n        - Dictionary containing BAM record information from the database, or None if no record is found.\n        - A generator that yields dictionaries containing BAM record information from the database.\n    \"\"\"\n    cursor = worker_state.get(\"db_cursor\")\n    assert cursor is not None, \"Database cursor is unexpectedly None\"\n\n    query = f\"\"\"SELECT\n                    read_id,\n                    parent_read_id,\n                    fast5_filepath,\n                    block_stride,\n                    called_events,\n                    mean_qscore,\n                    sequence_length,\n                    duration_template,\n                    first_sample_template,\n                    num_events_template,\n                    moves_table,\n                    read_fasta,\n                    read_quality,\n                    action,\n                    time_stamp\n                FROM fast5_bam_db\n                WHERE {id_type} = ?\"\"\"\n\n    # Execute the query with the provided ID value\n    cursor.execute(query, (id_value,))\n\n    # Define the columns\n    columns = [\n        \"read_id\",\n        \"parent_read_id\",\n        \"fast5_filepath\",\n        \"block_stride\",\n        \"called_events\",\n        \"mean_qscore\",\n        \"sequence_length\",\n        \"duration_template\",\n        \"first_sample_template\",\n        \"num_events_template\",\n        \"moves_table\",\n        \"read_fasta\",\n        \"read_quality\",\n        \"action\",\n        \"time_stamp\",\n    ]\n\n    if fetch_multiple:\n        # Fetch and yield the results one by one\n        return [dict(zip(columns, row)) for row in cursor.fetchall()]\n\n    else:\n        # Fetch the first result\n        row = cursor.fetchone()\n\n        # If a row is fetched, convert it to a dictionary\n        if row:\n            return dict(zip(columns, row))\n        else:\n            return None  # No record found for the given ID\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.make_fast5_files_list","title":"<code>make_fast5_files_list(directory: str) -&gt; List[str]</code>","text":"<p>Makes a list of all FAST5 files in a directory tree.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>str</code> <p>Path to the directory containing generated raw FAST5 files.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of all FAST5 file paths.</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def make_fast5_files_list(directory: str) -&gt; List[str]:\n    \"\"\"Makes a list of all FAST5 files in a directory tree.\n\n    Params:\n        directory (str): Path to the directory containing generated raw FAST5 files.\n\n    Returns:\n        List[str]: List of all FAST5 file paths.\n\n    \"\"\"\n    fast5_files = []\n    # os.walk generates the file names in a directory tree\n    for root, _, files in os.walk(directory):\n        for file in files:\n            if file.endswith(\".fast5\"):\n                # os.path.join constructs a full file path\n                full_path = os.path.join(root, file)\n                fast5_files.append(full_path)\n    return fast5_files\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.split_hdf5_signal_array","title":"<code>split_hdf5_signal_array(fileobj: Any, dataset_name: str, start_idx: int, end_idx: int) -&gt; None</code>","text":"<p>Splits the raw signal array of a read into two parts and stores them in a new dataset.</p> <p>Parameters:</p> Name Type Description Default <code>fileobj</code> <code>Any</code> <p>File object of the HDF5 file.</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset containing the raw signal array.</p> required <code>start_idx</code> <code>int</code> <p>Start index of the split.</p> required <code>end_idx</code> <code>int</code> <p>End index of the split.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def split_hdf5_signal_array(\n    fileobj: Any, dataset_name: str, start_idx: int, end_idx: int\n) -&gt; None:\n    \"\"\"\n    Splits the raw signal array of a read into two parts and stores them in a new dataset.\n\n    Params:\n        fileobj (Any): File object of the HDF5 file.\n        dataset_name (str): Name of the dataset containing the raw signal array.\n        start_idx (int): Start index of the split.\n        end_idx (int): End index of the split.\n\n    Returns:\n        None\n    \"\"\"\n    # Read the original dataset\n    original_dataset = fileobj[dataset_name]\n\n    # Perform the trim operation based on start and end indices\n    trimmed_data = original_dataset[start_idx:end_idx]  # +1 to include the end index\n\n    # Delete the original dataset\n    del fileobj[dataset_name]\n\n    # Create a new dataset with the same name and store the trimmed data\n    fileobj.create_dataset(dataset_name, data=trimmed_data)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.collate.split_raw_signal_for_duplex_reads","title":"<code>split_raw_signal_for_duplex_reads(worker_state: Dict[str, Any], fast5_filepath: str) -&gt; None</code>","text":"<p>Splits the raw signal of a duplex read into its child reads.</p> <p>Parameters:</p> Name Type Description Default <code>worker_state</code> <code>Dict[str, Any]</code> <p>Dictionary containing the database connection and cursor.</p> required <code>fast5_filepath</code> <code>str</code> <p>Path to the raw FAST5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/collate.py</code> <pre><code>def split_raw_signal_for_duplex_reads(\n    worker_state: Dict[str, Any], fast5_filepath: str\n) -&gt; None:\n    \"\"\"Splits the raw signal of a duplex read into its child reads.\n\n    Params:\n        worker_state (Dict[str, Any]): Dictionary containing the database connection and cursor.\n        fast5_filepath (str): Path to the raw FAST5 file.\n\n    Returns:\n        None\n\n    \"\"\"\n    data = extract_read_data_from_db_based_on_action(\n        fast5_filepath, action=\"duplicate\", worker_state=worker_state\n    )\n    if data is None:\n        return\n    with h5py.File(fast5_filepath, \"r+\") as fileobj:\n        for record in data:\n            read_id = record.get(\"read_id\")\n            dataset_name = f\"/read_{read_id}/Raw/Signal\"\n            split_point = record.get(\"split_point\")\n            first_sample_template = record.get(\"first_sample_template\")\n            if first_sample_template is None or split_point is None:\n                logger.error(\n                    \"Missing first_sample_template or split_point for read_id: {}\",\n                    read_id,\n                )\n                continue\n            first_sample_template += split_point\n\n            duration = record.get(\"duration_template\")\n            if duration &gt; first_sample_template:\n                # This is the first of the chimeric read's child\n                end_idx = duration\n            else:\n                # This is the second of the chimeric read's child\n                end_idx = first_sample_template + duration\n            # assert that end_idx is integer and greater than first_sample_template\n            assert isinstance(end_idx, int) and end_idx &gt; first_sample_template\n            split_hdf5_signal_array(\n                fileobj, dataset_name, start_idx=first_sample_template, end_idx=end_idx\n            )\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index","title":"<code>index</code>","text":""},{"location":"api_docs/#src.fast5_rekindler.index.DatabaseHandler","title":"<code>DatabaseHandler</code>","text":"<p>Database handler for multiprocessing.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>class DatabaseHandler:\n    \"\"\"Database handler for multiprocessing.\"\"\"\n\n    def __init__(self, output_dir: str, num_processes: int) -&gt; None:\n        self.output_dir = output_dir\n        self.num_processes = num_processes\n\n    def init_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Initializes the database connection for each worker.\"\"\"\n        unique_db_path = os.path.join(self.output_dir, f\"tmp_worker_{worker_id}.db\")\n        worker_state[\"db_connection\"] = sqlite3.connect(unique_db_path)\n        worker_state[\"db_cursor\"] = worker_state[\"db_connection\"].cursor()\n        worker_state[\"db_cursor\"].execute(TABLE_INIT_QUERY)\n        worker_state[\"db_connection\"].commit()\n\n    def exit_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n        \"\"\"Closes the database connection for each worker.\"\"\"\n        conn = worker_state[\"db_connection\"]\n        cursor = worker_state[\"db_cursor\"]\n        conn.commit()\n        cursor.close()\n        conn.close()\n\n    def merge_databases(self) -&gt; None:\n        \"\"\"Merges the databases from each worker into a single database.\"\"\"\n        db_path = os.path.join(self.output_dir, \"index_db.db\")\n        main_conn = sqlite3.connect(db_path)\n        main_cursor = main_conn.cursor()\n        main_cursor.execute(TABLE_INIT_QUERY)\n        for i in range(self.num_processes):\n            worker_db_path = os.path.join(self.output_dir, f\"tmp_worker_{i}.db\")\n            main_cursor.execute(f\"ATTACH DATABASE '{worker_db_path}' AS worker_db\")\n            main_cursor.execute(\"BEGIN\")\n            main_cursor.execute(\n                \"\"\"\n                INSERT OR IGNORE INTO index_db\n                SELECT * FROM worker_db.index_db\n            \"\"\"\n            )\n            main_cursor.execute(\"COMMIT\")\n            main_cursor.execute(\"DETACH DATABASE worker_db\")\n            os.remove(worker_db_path)\n        main_conn.commit()\n        main_conn.close()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.DatabaseHandler.exit_func","title":"<code>exit_func(worker_id: int, worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Closes the database connection for each worker.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def exit_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Closes the database connection for each worker.\"\"\"\n    conn = worker_state[\"db_connection\"]\n    cursor = worker_state[\"db_cursor\"]\n    conn.commit()\n    cursor.close()\n    conn.close()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.DatabaseHandler.init_func","title":"<code>init_func(worker_id: int, worker_state: Dict[str, Any]) -&gt; None</code>","text":"<p>Initializes the database connection for each worker.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def init_func(self, worker_id: int, worker_state: Dict[str, Any]) -&gt; None:\n    \"\"\"Initializes the database connection for each worker.\"\"\"\n    unique_db_path = os.path.join(self.output_dir, f\"tmp_worker_{worker_id}.db\")\n    worker_state[\"db_connection\"] = sqlite3.connect(unique_db_path)\n    worker_state[\"db_cursor\"] = worker_state[\"db_connection\"].cursor()\n    worker_state[\"db_cursor\"].execute(TABLE_INIT_QUERY)\n    worker_state[\"db_connection\"].commit()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.DatabaseHandler.merge_databases","title":"<code>merge_databases() -&gt; None</code>","text":"<p>Merges the databases from each worker into a single database.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def merge_databases(self) -&gt; None:\n    \"\"\"Merges the databases from each worker into a single database.\"\"\"\n    db_path = os.path.join(self.output_dir, \"index_db.db\")\n    main_conn = sqlite3.connect(db_path)\n    main_cursor = main_conn.cursor()\n    main_cursor.execute(TABLE_INIT_QUERY)\n    for i in range(self.num_processes):\n        worker_db_path = os.path.join(self.output_dir, f\"tmp_worker_{i}.db\")\n        main_cursor.execute(f\"ATTACH DATABASE '{worker_db_path}' AS worker_db\")\n        main_cursor.execute(\"BEGIN\")\n        main_cursor.execute(\n            \"\"\"\n            INSERT OR IGNORE INTO index_db\n            SELECT * FROM worker_db.index_db\n        \"\"\"\n        )\n        main_cursor.execute(\"COMMIT\")\n        main_cursor.execute(\"DETACH DATABASE worker_db\")\n        os.remove(worker_db_path)\n    main_conn.commit()\n    main_conn.close()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.build_index_db","title":"<code>build_index_db(fast5_dir: str, output_dir: str, num_processes: int) -&gt; None</code>","text":"<p>Builds an index mapping read_ids to FAST5 file paths.</p> <p>Parameters:</p> Name Type Description Default <code>fast5_dir</code> <code>str</code> <p>Path to a FAST5 file or directory of FAST5 files.</p> required <code>output_dir</code> <code>str</code> <p>Path to a output directory.</p> required <code>num_processes</code> <code>int</code> <p>Number of processes to use.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def build_index_db(fast5_dir: str, output_dir: str, num_processes: int) -&gt; None:\n    \"\"\"\n    Builds an index mapping read_ids to FAST5 file paths.\n\n    Params:\n        fast5_dir (str): Path to a FAST5 file or directory of FAST5 files.\n        output_dir (str): Path to a output directory.\n        num_processes (int): Number of processes to use.\n\n    Returns:\n        None\n    \"\"\"\n    # Ensure output directory exists\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    total_files = sum(1 for _ in generate_fast5_file_paths(fast5_dir))\n\n    num_processes = min(num_processes, total_files)\n    db_handler = DatabaseHandler(output_dir, num_processes)\n\n    with WorkerPool(\n        n_jobs=num_processes, use_worker_state=True, pass_worker_id=True\n    ) as pool:\n        pool.map(\n            build_index_db_worker,\n            [\n                (fast5_filepath,)\n                for fast5_filepath in generate_fast5_file_paths(fast5_dir)\n            ],\n            iterable_len=total_files,\n            worker_init=db_handler.init_func,\n            worker_exit=db_handler.exit_func,\n            progress_bar=True,\n            progress_bar_options={\"desc\": \"\", \"unit\": \"files\", \"colour\": \"green\"},\n        )\n    db_handler.merge_databases()\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.build_index_db_worker","title":"<code>build_index_db_worker(worker_id: int, worker_state: Dict[str, Any], fast5_filepath: str) -&gt; None</code>","text":"<p>Builds an index mapping read_ids to FAST5 file paths. Every worker has its own database.</p> <p>Parameters:</p> Name Type Description Default <code>fast5_filepath</code> <code>str</code> <p>Path to a FAST5 file.</p> required <code>worker_id</code> <code>int</code> <p>Worker ID.</p> required <code>worker_state</code> <code>Dict[str, Any]</code> <p>Worker state dictionary.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def build_index_db_worker(\n    worker_id: int, worker_state: Dict[str, Any], fast5_filepath: str\n) -&gt; None:\n    \"\"\"\n    Builds an index mapping read_ids to FAST5 file paths. Every worker has its own database.\n\n    Params:\n        fast5_filepath (str): Path to a FAST5 file.\n        worker_id (int): Worker ID.\n        worker_state (Dict[str, Any]): Worker state dictionary.\n\n    Returns:\n        None\n    \"\"\"\n    read_ids = get_readids(fast5_filepath)\n    data = prepare_data(read_ids, fast5_filepath)\n    write_database(data, worker_state[\"db_cursor\"], worker_state[\"db_connection\"])\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.find_database_size","title":"<code>find_database_size(output_dir: str) -&gt; Any</code>","text":"<p>Find the number of records in the database.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Path to the database directory.</p> required <p>Returns:</p> Name Type Description <code>size</code> <code>Any</code> <p>Number of records in the database.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def find_database_size(output_dir: str) -&gt; Any:\n    \"\"\"\n    Find the number of records in the database.\n\n    Params:\n        output_dir (str): Path to the database directory.\n\n    Returns:\n        size (Any): Number of records in the database.\n    \"\"\"\n    database_path = os.path.join(output_dir, \"index_db.db\")\n    conn = sqlite3.connect(database_path)\n    cursor = conn.cursor()\n    cursor.execute(\"SELECT COUNT(*) FROM index_db\")\n    size = cursor.fetchone()[0]\n    return size\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.generate_fast5_file_paths","title":"<code>generate_fast5_file_paths(fast5_dir: str) -&gt; Generator</code>","text":"<p>Traverse the directory and yield all the fast5 file paths.</p> <p>Parameters:</p> Name Type Description Default <code>fast5_dir</code> <code>str</code> <p>Path to a FAST5 file or directory of FAST5 files.</p> required <p>Returns:</p> Name Type Description <code>pod5_filepath</code> <code>str</code> <p>Path to a FAST5 file.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def generate_fast5_file_paths(fast5_dir: str) -&gt; Generator:\n    \"\"\"Traverse the directory and yield all the fast5 file paths.\n\n    Params:\n        fast5_dir (str): Path to a FAST5 file or directory of FAST5 files.\n\n    Returns:\n        pod5_filepath (str): Path to a FAST5 file.\n    \"\"\"\n\n    if os.path.isdir(fast5_dir):\n        for root, _, files in os.walk(fast5_dir):\n            for file in files:\n                if file.endswith(\".fast5\"):\n                    yield os.path.join(root, file)\n    elif os.path.isfile(fast5_dir) and fast5_dir.endswith(\".fast5\"):\n        yield fast5_dir\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.get_readids","title":"<code>get_readids(fast5_filepath: str) -&gt; List[str]</code>","text":"<p>Get a list of read_ids from a FAST5 file.</p> <p>Parameters:</p> Name Type Description Default <code>fast5_filepath</code> <code>str</code> <p>Path to a FAST5 file.</p> required <p>Returns:</p> Name Type Description <code>read_ids</code> <code>List[str]</code> <p>List of read_ids.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def get_readids(fast5_filepath: str) -&gt; List[str]:\n    \"\"\"\n    Get a list of read_ids from a FAST5 file.\n\n    Params:\n        fast5_filepath (str): Path to a FAST5 file.\n\n    Returns:\n        read_ids (List[str]): List of read_ids.\n    \"\"\"\n    read_ids = []\n    with get_fast5_file(fast5_filepath, mode=\"r\") as f5:\n        for read in f5.get_reads():\n            read_ids.append(read.read_id)\n    return read_ids\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.prepare_data","title":"<code>prepare_data(read_ids: List[str], fast5_filepath: str) -&gt; List[Tuple[str, str]]</code>","text":"<p>Prepare tuples for read_id and fast5_filepath for insertion into database.</p> <p>Parameters:</p> Name Type Description Default <code>read_ids</code> <code>List[str]</code> <p>List of read_ids.</p> required <code>fast5_filepath</code> <code>str</code> <p>Path to a FAST5 file.</p> required <p>Returns:</p> Name Type Description <code>data</code> <code>List[Tuple[str, str]]</code> <p>List of tuples of read_id, fast5_filepath</p> <code>List[Tuple[str, str]]</code> <p>and other relevant read data.</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def prepare_data(read_ids: List[str], fast5_filepath: str) -&gt; List[Tuple[str, str]]:\n    \"\"\"\n    Prepare tuples for read_id and fast5_filepath for insertion into database.\n\n    Params:\n        read_ids (List[str]): List of read_ids.\n        fast5_filepath (str): Path to a FAST5 file.\n\n    Returns:\n        data (List[Tuple[str, str]]): List of tuples of read_id, fast5_filepath\n        and other relevant read data.\n    \"\"\"\n    return [(read_id, fast5_filepath) for read_id in read_ids]\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.index.write_database","title":"<code>write_database(data: List[Tuple[str, str]], cursor: sqlite3.Cursor, conn: sqlite3.Connection) -&gt; None</code>","text":"<p>Write the index data (read_id, filepath) to the database.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Tuple[str, str, int, int, float, int, int, int, int]]</code> <p>List of tuples read_id and filepath</p> required <code>cursor</code> <code>Cursor</code> <p>Cursor object for the database.</p> required <code>conn</code> <code>Connection</code> <p>Connection object for the database.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/index.py</code> <pre><code>def write_database(\n    data: List[Tuple[str, str]], cursor: sqlite3.Cursor, conn: sqlite3.Connection\n) -&gt; None:\n    \"\"\"\n    Write the index data (read_id, filepath) to the database.\n\n    Params:\n        data (List[Tuple[str, str, int, int, float, int, int, int, int]]): List of tuples read_id and filepath\n        cursor (sqlite3.Cursor): Cursor object for the database.\n        conn (sqlite3.Connection): Connection object for the database.\n\n    Returns:\n        None\n    \"\"\"\n    cursor.executemany(\n        \"INSERT OR IGNORE INTO index_db (read_id, fast5_filepath) VALUES (?, ?)\", data\n    )\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.join_dbs","title":"<code>join_dbs</code>","text":""},{"location":"api_docs/#src.fast5_rekindler.join_dbs.join_databases","title":"<code>join_databases(output_dir: str) -&gt; str</code>","text":"<p>Merges the index_db and bam_db databases into a single output database fast5_bam_db.db.</p> <p>Parameters:</p> Name Type Description Default <code>output_dir</code> <code>str</code> <p>Path to the output directory.</p> required <p>Returns:</p> Name Type Description <code>output_db_path</code> <code>str</code> <p>Path to the output database.</p> Source code in <code>src/fast5_rekindler/join_dbs.py</code> <pre><code>def join_databases(output_dir: str) -&gt; str:\n    \"\"\"Merges the index_db and bam_db databases into a\n    single output database fast5_bam_db.db.\n\n    Params:\n        output_dir (str): Path to the output directory.\n\n    Returns:\n        output_db_path (str): Path to the output database.\n    \"\"\"\n    bam_db_path = os.path.join(output_dir, \"bam_db.db\")\n    index_db_path = os.path.join(output_dir, \"index_db.db\")\n    output_db_path = os.path.join(output_dir, \"fast5_bam_db.db\")\n\n    if os.path.exists(output_db_path):\n        os.remove(output_db_path)\n    # Create output database connection\n    conn_output = sqlite3.connect(output_db_path)\n    cursor_output = conn_output.cursor()\n\n    # Create the table in the output database\n    cursor_output.execute(\n        \"\"\"CREATE TABLE IF NOT EXISTS fast5_bam_db (\n                            read_id TEXT PRIMARY KEY,\n                            parent_read_id TEXT,\n                            fast5_filepath TEXT,\n                            block_stride INTEGER,\n                            called_events INTEGER,\n                            mean_qscore FLOAT,\n                            sequence_length INTEGER,\n                            duration_template INTEGER,\n                            first_sample_template INTEGER,\n                            split_point INTEGER,\n                            num_events_template INTEGER,\n                            moves_table BLOB,\n                            read_fasta TEXT,\n                            read_quality TEXT,\n                            time_stamp TIMESTAMP,\n                            action TEXT\n                        )\"\"\"\n    )\n\n    # Attach both databases to the new database connection\n    cursor_output.execute(f\"ATTACH DATABASE '{index_db_path}' AS idx\")\n    cursor_output.execute(f\"ATTACH DATABASE '{bam_db_path}' AS bam\")\n\n    # Perform the JOIN operation and insert the data into the new database\n    # - First join selects primary reads\n    # - Second join selects duplex reads (those reads that have a common parent read\n    # - Third join selects reads that are in FAST5 files but not in the BAM file)\n    cursor_output.execute(\n        \"\"\"\n        INSERT INTO fast5_bam_db\n        SELECT * FROM (\n            SELECT b.read_id,\n                b.parent_read_id,\n                i.fast5_filepath,\n                b.block_stride,\n                b.called_events,\n                b.mean_qscore,\n                b.sequence_length,\n                b.duration_template,\n                b.first_sample_template,\n                b.split_point,\n                b.num_events_template,\n                b.moves_table,\n                b.read_fasta,\n                b.read_quality,\n                b.time_stamp,\n                'process' AS action\n            FROM bam.bam_db AS b\n            INNER JOIN idx.index_db AS i ON b.read_id = i.read_id\n\n            UNION ALL\n\n            SELECT b.read_id,\n                b.parent_read_id,\n                i.fast5_filepath,\n                b.block_stride,\n                b.called_events,\n                b.mean_qscore,\n                b.sequence_length,\n                b.duration_template,\n                b.first_sample_template,\n                b.split_point,\n                b.num_events_template,\n                b.moves_table,\n                b.read_fasta,\n                b.read_quality,\n                b.time_stamp,\n                'duplicate' AS action\n            FROM bam.bam_db AS b\n            INNER JOIN idx.index_db AS i ON b.parent_read_id = i.read_id\n\n            UNION ALL\n\n            SELECT i.read_id,\n                NULL AS parent_read_id,\n                i.fast5_filepath,\n                NULL AS block_stride,\n                NULL AS called_events,\n                NULL AS mean_qscore,\n                NULL AS sequence_length,\n                NULL AS duration_template,\n                NULL AS first_sample_template,\n                NULL AS split_point,\n                NULL AS num_events_template,\n                NULL AS moves_table,\n                NULL AS read_fasta,\n                NULL AS read_quality,\n                NULL AS time_stamp,\n                'delete' AS action\n            FROM idx.index_db AS i\n            LEFT JOIN bam.bam_db AS b ON i.read_id = b.read_id\n            WHERE b.read_id IS NULL\n\n\n        )\n        \"\"\"\n    )\n\n    # Commit the changes\n    conn_output.commit()\n\n    # Check if the data has been inserted\n    cursor_output.execute(\"SELECT COUNT(*) FROM fast5_bam_db\")\n    row_count = cursor_output.fetchone()[0]\n    if row_count &gt; 0:\n        # If data is inserted successfully, delete the original databases\n        os.remove(index_db_path)\n        os.remove(bam_db_path)\n    else:\n        print(\"No data was inserted. Original databases have not been removed.\")\n\n    # Close the connection\n    cursor_output.close()\n    conn_output.close()\n\n    return output_db_path\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.logger_config","title":"<code>logger_config</code>","text":""},{"location":"api_docs/#src.fast5_rekindler.logger_config.configure_logger","title":"<code>configure_logger(new_log_directory: str) -&gt; str</code>","text":"<p>Configure the logger to log to a file.</p> <p>Parameters:</p> Name Type Description Default <code>new_log_directory</code> <code>str</code> <p>Path to the directory where the log file will be saved.</p> required <p>Returns:</p> Name Type Description <code>log_filepath</code> <code>str</code> <p>Path to the log file.</p> Source code in <code>src/fast5_rekindler/logger_config.py</code> <pre><code>def configure_logger(new_log_directory: str) -&gt; str:\n    \"\"\"Configure the logger to log to a file.\n\n    Params:\n        new_log_directory (str): Path to the directory where the log file will be saved.\n\n    Returns:\n        log_filepath (str): Path to the log file.\n    \"\"\"\n    global log_directory  # Declare log_directory as global to modify it\n\n    if new_log_directory is not None:\n        log_directory = (\n            new_log_directory  # Update log directory if a new path is provided\n        )\n\n    # Ensure log directory exists\n    os.makedirs(log_directory, exist_ok=True)\n    if not os.path.exists(log_directory):\n        os.makedirs(log_directory)\n\n    # Get current date and time\n    now = datetime.now()\n    timestamp = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n\n    # Use the timestamp in the log file name\n    app_version = get_version()\n    log_filename = f\"fast5_rekindler_v{app_version}_{timestamp}.log\"\n\n    log_filepath = os.path.join(log_directory, log_filename)\n\n    # Configure logger to log to the file\n    # No need to call logger.remove() as we want to keep the default stderr handler\n    logger.add(log_filepath, format=\"{time} {level} {message}\")\n\n    # Now logs will be sent to both the terminal and log_filename\n    logger.opt(depth=1).info(f\"Started FAST5 Rekindler v({app_version})\\n\")\n    return log_filepath\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.logger_config.get_version","title":"<code>get_version() -&gt; Any</code>","text":"<p>Get the version of the app from pyproject.toml.</p> <p>Returns:</p> Name Type Description <code>app_version</code> <code>Any</code> <p>Version of the app.</p> Source code in <code>src/fast5_rekindler/logger_config.py</code> <pre><code>def get_version() -&gt; Any:\n    \"\"\"Get the version of the app from pyproject.toml.\n\n    Returns:\n        app_version (Any): Version of the app.\n    \"\"\"\n    app_version = version(\"fast5_rekindler\")\n    return app_version\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.raw_fast5","title":"<code>raw_fast5</code>","text":""},{"location":"api_docs/#src.fast5_rekindler.raw_fast5.convert_pod5_to_raw_fast5","title":"<code>convert_pod5_to_raw_fast5(input_dir: str, output_dir: str, num_processes: int = 4) -&gt; None</code>","text":"<p>Converts all pod5 files in input_dir to raw fast5 files and saves them in output_dir.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to the directory containing the pod5 files.</p> required <code>output_dir</code> <code>str</code> <p>Path to the output directory.</p> required <code>num_processes</code> <code>int</code> <p>Number of processes to use. Default is 4.</p> <code>4</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/raw_fast5.py</code> <pre><code>def convert_pod5_to_raw_fast5(\n    input_dir: str, output_dir: str, num_processes: int = 4\n) -&gt; None:\n    \"\"\"\n    Converts all pod5 files in input_dir to raw fast5 files and saves them in output_dir.\n\n    Params:\n        input_dir (str): Path to the directory containing the pod5 files.\n        output_dir (str): Path to the output directory.\n        num_processes (int): Number of processes to use. Default is 4.\n\n    Returns:\n        None\n    \"\"\"\n    p5_f5_filepaths, num_pod5_files = make_pod5_file_list(input_dir, output_dir)\n\n    with WorkerPool(n_jobs=min(num_processes, num_pod5_files)) as pool:\n        pool.map(\n            convert_pod5_to_raw_fast5_worker,\n            [\n                (pod5_filepath, fast5_filepath)\n                for pod5_filepath, fast5_filepath in p5_f5_filepaths\n            ],\n            iterable_len=num_pod5_files,\n            progress_bar=True,\n            progress_bar_options={\"desc\": \"\", \"unit\": \"files\", \"colour\": \"green\"},\n        )\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.raw_fast5.convert_pod5_to_raw_fast5_worker","title":"<code>convert_pod5_to_raw_fast5_worker(pod5_filepath: str, fast5_filepath: str) -&gt; None</code>","text":"<p>Converts a single pod5 file to a raw fast5 file.</p> <p>Parameters:</p> Name Type Description Default <code>pod5_filepath</code> <code>str</code> <p>Path to the pod5 file.</p> required <code>fast5_filepath</code> <code>str</code> <p>Path to the output fast5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/fast5_rekindler/raw_fast5.py</code> <pre><code>def convert_pod5_to_raw_fast5_worker(pod5_filepath: str, fast5_filepath: str) -&gt; None:\n    \"\"\"Converts a single pod5 file to a raw fast5 file.\n\n    Params:\n        pod5_filepath (str): Path to the pod5 file.\n        fast5_filepath (str): Path to the output fast5 file.\n\n    Returns:\n        None\n    \"\"\"\n    # Fetch all the read ids in the POD5 file\n    with p5.Reader(pod5_filepath) as reader:\n        read_ids = reader.read_ids\n    # Convert\n    convert_pod5_to_fast5(Path(pod5_filepath), Path(fast5_filepath), read_ids)\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.raw_fast5.get_pod5_filepath","title":"<code>get_pod5_filepath(read_id: str, db_cursor: sqlite3.Cursor) -&gt; Any</code>","text":"<p>Fetches the pod5 filepath for a given read_id from the SQLite index database.</p> <p>Parameters:</p> Name Type Description Default <code>read_id</code> <code>str</code> <p>str The read_id of the read to be extracted.</p> required <code>db_cursor</code> <code>Cursor</code> <p>sqlite3.Cursor Cursor object for the database.</p> required <p>Returns:     pod5_filepath: str         Path to the pod5 file.</p> Source code in <code>src/fast5_rekindler/raw_fast5.py</code> <pre><code>def get_pod5_filepath(read_id: str, db_cursor: sqlite3.Cursor) -&gt; Any:\n    \"\"\"Fetches the pod5 filepath for a given read_id\n    from the SQLite index database.\n\n    Params:\n        read_id: str\n            The read_id of the read to be extracted.\n        db_cursor: sqlite3.Cursor\n            Cursor object for the database.\n    Returns:\n        pod5_filepath: str\n            Path to the pod5 file.\n    \"\"\"\n    db_cursor.execute(\"SELECT pod5_filepath FROM data WHERE read_id = ?\", (read_id,))\n    result = db_cursor.fetchone()\n    return result[0] if result else None\n</code></pre>"},{"location":"api_docs/#src.fast5_rekindler.raw_fast5.make_pod5_file_list","title":"<code>make_pod5_file_list(input_dir: str, output_dir: str) -&gt; Tuple[Iterator[Tuple[str, str]], int]</code>","text":"<p>Recursively find all pod5 files (*.pod5) in input_dir and return a list of Path objects.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>Path to the directory containing the pod5 files.</p> required <p>Returns:</p> Name Type Description <code>pod5_file_list</code> <code>Tuple[Iterator[Tuple[str, str]], int]</code> <p>A tuple of zipped pod5 and fast5 filepaths and total number of pod5 files.</p> Source code in <code>src/fast5_rekindler/raw_fast5.py</code> <pre><code>def make_pod5_file_list(\n    input_dir: str, output_dir: str\n) -&gt; Tuple[Iterator[Tuple[str, str]], int]:\n    \"\"\"Recursively find all pod5 files (*.pod5) in input_dir and return a list of Path objects.\n\n    Params:\n        input_dir (str): Path to the directory containing the pod5 files.\n\n    Returns:\n        pod5_file_list (Tuple[Iterator[Tuple[str, str]], int]): A tuple of zipped pod5 and fast5 filepaths and total number of pod5 files.\n    \"\"\"\n\n    pod5_file_list = []\n    fast5_file_list = []\n    for root, _, files in os.walk(input_dir):\n        for file in files:\n            if file.endswith(\".pod5\"):\n                # Construct the full path of the .fast5 file\n                pod5_output_path = os.path.join(root, file)\n\n                # Construct the corresponding .pod5 file path in the output directory\n                relative_path = os.path.relpath(root, input_dir)\n                fast5_output_path = os.path.join(\n                    output_dir, relative_path, file.replace(\".pod5\", \".fast5\")\n                )\n\n                # Create the subdirectory in the output directory if it doesn't exist\n                os.makedirs(os.path.dirname(fast5_output_path), exist_ok=True)\n\n                # Add the paths to the lists\n                pod5_file_list.append(pod5_output_path)\n                fast5_file_list.append(fast5_output_path)\n    return zip(pod5_file_list, fast5_file_list), len(pod5_file_list)\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#006-2024-01-12","title":"0.0.6 - 2024-01-12","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>Removed redundant code that gave error when closing the database</li> </ul>"},{"location":"changelog/#005-2024-01-12","title":"0.0.5 - 2024-01-12","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>A bug where some processes created emtpy tmp_worker files that caused problems during merging them in bam.db</li> </ul>"},{"location":"changelog/#004-2023-12-13","title":"0.0.4 - 2023-12-13","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>A bug that caused tailfindr to crash because the basecalling subgroup name had a typo</li> </ul>"},{"location":"changelog/#003-2023-12-05","title":"0.0.3 - 2023-12-05","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Splitting of duplex reads in FAST5</li> </ul>"},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Errors encountered in suplementary alignments due to missing FASTA data</li> <li>CLI text</li> <li>Fixed missing BAM sorting step</li> </ul>"},{"location":"changelog/#002-2023-11-16","title":"0.0.2 - 2023-11-16","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>First fully functional release of code which can collate info in POD5 and BAM file into legacy basecalled FAST5 files</li> </ul>"}]}